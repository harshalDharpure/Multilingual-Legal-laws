## Human-Centered Evaluation Protocol (Stakeholder Study)

Addresses: Reviewer #1 (stakeholder validation), Reviewer #6 (explanation faithfulness vs plausibility)

Participants:
- 3–5 frontline NGO/paralegal workers (CSA support)
- 2–3 legal practitioners (JJ/POCSO experience)
- 2 postgraduate law students (annotators)

Task:
- Evaluate 200 items (stratified), half canonical, half user-style (CALSD-Real).
- For each: read question+answer+reasoning+citations and rate on 1–5 scales:
  - Legal Correctness
  - Statutory Grounding (traceable to provisions)
  - Usefulness for Layperson (actionability/clarity)
  - Harm Avoidance (no unsafe advice)
  - Faithfulness of Explanation (no hallucinated legal rules)

Artifacts Provided:
- Example guide with positive/negative exemplars, boundary cases.
- Definitions of each dimension, with decision rubrics.

Process Controls:
- Double-blind; raters do not see which pipeline stage produced the answer.
- Independent ratings; no cross-communication during labeling.
- Adjudication by senior legal reviewer only for guideline updates.

Agreement Reporting:
- Pairwise Cohen’s kappa per dimension.
- Rater vs. meta-adjudicator agreement on a 10% audit subset.

Outcomes to Report in Paper:
- Mean±SD per dimension; between-group (canonical vs user-style) comparisons.
- Correlation between human ratings and NLI entailment.
- Qualitative error taxonomy (e.g., missing mandatory reporting, misapplied consent).


